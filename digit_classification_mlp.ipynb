{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP for Handwritten Digit Classification\n",
    "\n",
    "Objective: Implement a multi-layer perceptron (MLP) to classify handwritten digits from the scikit-learn Digits dataset. Evaluate how different activation functions affect early learning speed, convergence, and final test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing\n",
    "\n",
    "The dataset is loaded from scikit-learn and preprocessed by:\n",
    "- Normalising the input features\n",
    "- Splitting into training and test sets (80-20)\n",
    "- Wrapping the data in PyTorch DataLoaders for batching.\n",
    "\n",
    "This ensures the MLP receives data in a format suitable for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 6573,
     "status": "ok",
     "timestamp": 1716933264679,
     "user": {
      "displayName": "Cheryl C",
      "userId": "07083983319276283397"
     },
     "user_tz": -720
    },
    "id": "aKAxsm3DtLcE"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Data Loading and Preprocessing\n",
    "\n",
    "# Load the Digits dataset\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "torch.manual_seed(231)\n",
    "np.random.seed(231)\n",
    "\n",
    "# Split the data into training and test sets (80%-20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=231)\n",
    "\n",
    "# Normalise the images by scaling pixel values to 0-1 range\n",
    "normaliser = MinMaxScaler()\n",
    "# Normaliser is fitted to the training data\n",
    "X_train_normalised = normaliser.fit_transform(X_train) \n",
    "# Normaliser fit to training set applied to test set\n",
    "X_test_normalised = normaliser.transform(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Converting NumPy arrays into PyTorch tensors to allow for multi-dimensional data rep. & efficient computation\n",
    "Using .float() for features since NN computations are more precise with floats.\n",
    "\"\"\"\n",
    "X_train_tensor = torch.from_numpy(X_train_normalised).float()\n",
    "# .long() for target values, since Pytorch loss expects 'long' format\n",
    "y_train_tensor = torch.from_numpy(y_train).long()\n",
    "X_test_tensor = torch.from_numpy(X_test_normalised).float()\n",
    "y_test_tensor = torch.from_numpy(y_test).long()\n",
    "\n",
    "# Create DataLoader objects for both training and test sets\n",
    "\"\"\"\n",
    "Batch size 64 provides enough variability to aid generalisation,\n",
    "and enough samples to calculate stable gradient estimates for smoother training.\n",
    "\"\"\"\n",
    "batch_size = 64\n",
    "\n",
    "# TensorDataset() bundles multiple tensors into a single dataset\n",
    "training_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "# DataLoader divides the dataset into smaller batches to allow for more frequent weight updates (once per batch).\n",
    "# For training data, shuffle=true to improve generalisation by preventing the model from learning artifical order-related data patterns.\n",
    "training_loader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "# For test data, shuffle=false to prevent introduction of randomness that isn't reflective of how the model would be used in real-world scenarios.\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Model Architecture\n",
    "\n",
    "A simple multilayer perceptron is implemented with an input layer, one hidden layer, and an output layer. It accepts an activation function as a parameter to enable later experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1716933270101,
     "user": {
      "displayName": "Cheryl C",
      "userId": "07083983319276283397"
     },
     "user_tz": -720
    },
    "id": "v32tazy4xyMN"
   },
   "outputs": [],
   "source": [
    "# MLP Model Implementation\n",
    "\n",
    "\"\"\"\n",
    "Defining a neural network class by extending torch.nn.Module.\n",
    "NN has one input layer with 64 neurons (since Digits dataset is 8x8 images), one hidden layer of 128 neurons, \n",
    "and one output layer with 10 neurons (since there are ten possible digit options).\n",
    "It accepts an activation function as a parameter.\n",
    "\"\"\"\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "torch.manual_seed(231)\n",
    "np.random.seed(231)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# NeuralNetwork inherits from torch.nn.Module\n",
    "class NeuralNetwork(torch.nn.Module):\n",
    "  def __init__(self, activation_function):\n",
    "    super().__init__() # super() calls the torch.nn.Module constructor\n",
    "    # Define the three layers using torch.nn.Linear (creates fully connected/linear layers)\n",
    "    # Input layer has 64 neurons (64 features) and transforms that input data into 128 neurons\n",
    "    self.input_layer = torch.nn.Linear(64, 128)\n",
    "    # Hidden layer with 128 neurons in and out\n",
    "    self.hidden_layer = torch.nn.Linear(128, 128)\n",
    "    # Output layer with 10 neurons out that correspond to the 10 classes (digits 0-9)\n",
    "    self.output_layer = torch.nn.Linear(128, 10)\n",
    "    self.activation_function = activation_function\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Forward pass through the neural network\n",
    "    x = self.activation_function(self.input_layer(x))\n",
    "    x = self.activation_function(self.hidden_layer(x))\n",
    "    softmax = torch.nn.Softmax(dim=1)\n",
    "    x = softmax(self.output_layer(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training\n",
    "\n",
    "The `train_show` function trains the MLP for a set number of epochs using CrossEntropyLoss and the Adam optimiser. It prints the average training loss and accuracy per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5945,
     "status": "ok",
     "timestamp": 1716933281362,
     "user": {
      "displayName": "Cheryl C",
      "userId": "07083983319276283397"
     },
     "user_tz": -720
    },
    "id": "HRlITv9r8l98",
    "outputId": "da5e9e4c-9ee6-47d6-9399-ecba2a52527e"
   },
   "outputs": [],
   "source": [
    "# MLP Model Training \n",
    "\n",
    "# Hyperparameter tuning: Chose CrossEntropyLoss as loss function, decided on Adam optimiser over SGD, and set an appropriate learning rate. \n",
    "\n",
    "def train_show(network, lossFunc, optimiser, epochs):\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "      # Storing every calculated loss and accuracy in the epoch to calculate average\n",
    "      lossHistory = []\n",
    "      accuracyHistory = []\n",
    "      \n",
    "      network.train() # Set model to training mode, which has active dropout to make the model more robust and prevent overfitting\n",
    "        \n",
    "      for data, targ in training_loader:\n",
    "        optimiser.zero_grad() # PyTorch gradients accumulate by default, so they need to be zeroed out before each forward pass to avoid mixing between batches.\n",
    "\n",
    "        y = network.forward(data) # Perform forward pass by calling forward() in NeuralNetwork class\n",
    "        \n",
    "        loss = lossFunc(y,targ) # Calculate the loss\n",
    "        loss.backward() # Runs autograd to get the gradients needed by the optimiser: Computes gradients of the loss with respect to all model parameters using backpropagation.\n",
    "\n",
    "        optimiser.step() # Takes a step: updates the model parameters in the direction that minimises the loss using the calculated gradients.\n",
    "\n",
    "        \"\"\"\n",
    "        torch.argmax(y,dim=1) returns the index of the class with the highest\n",
    "        probability for each input sample in the batch, which is then compared\n",
    "        to the actual (targ) class value for that sample. A boolean tensor is\n",
    "        returned to indicate whether the prediction is correct or not,\n",
    "        and .float() converts that boolean tensor into a float tensor\n",
    "        (1 for correct, 0 for incorrect).\n",
    "        torch.mean() calculates the mean of all the input samples' float tensors\n",
    "        to effectively get the proportion of correct predictions for that batch.\n",
    "        \"\"\"\n",
    "        accuracy = torch.mean((torch.argmax(y,dim=1) == targ).float())\n",
    "          \n",
    "        # Add the loss and accuracy values to their lists. These lists will later be used to calculate the average over the epoch.\n",
    "        lossHistory.append(loss.detach().item()) # Extracting the loss value as a Python scalar.\n",
    "        accuracyHistory.append(accuracy.detach()) # Detaches the accuracy tensor from the computation graph\n",
    "\n",
    "      # Calculate average loss and accuracy for the current epoch\n",
    "      avg_loss = sum(lossHistory) / len(lossHistory)\n",
    "      avg_accuracy = sum(accuracyHistory) / len(accuracyHistory)\n",
    "\n",
    "      # Print average training loss and accuracy over the epoch\n",
    "      print(f\"Epoch {epoch+1}: Loss = {avg_loss:.3f}, Accuracy = {int(avg_accuracy*100)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation\n",
    "\n",
    "The `test_network` function evaluates a trained MLP on the test set and prints the final test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 430,
     "status": "ok",
     "timestamp": 1716933285671,
     "user": {
      "displayName": "Cheryl C",
      "userId": "07083983319276283397"
     },
     "user_tz": -720
    },
    "id": "lJ8xRLJr8oKz",
    "outputId": "eb4a5cf2-bb67-4dd9-e454-2d4d27d8b260"
   },
   "outputs": [],
   "source": [
    "# Model Evaluation on Test Set\n",
    "\n",
    "# Measuring the model's accuracy on the test set.\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "torch.manual_seed(231)\n",
    "np.random.seed(231)\n",
    "\n",
    "def test_network(network):\n",
    "  network.eval() # Set the network to evaluation mode, which disables dropout and normalises data using running mean and variance estimates collected during model training\n",
    "  \n",
    "  # Storing every calculated loss and accuracy to calculate the average\n",
    "  lossHistory = []\n",
    "  accuracyHistory = []\n",
    "  \n",
    "  with torch.no_grad(): # Disable gradient computation\n",
    "    for test_data, test_targ in test_loader:\n",
    "      y_test = network(test_data) # Perform forward pass\n",
    "      test_loss = lossFunction(y_test, test_targ) # Compute the loss\n",
    "      test_accuracy = torch.mean((torch.argmax(y_test,dim=1) == test_targ).float()) # Compute the accuracy\n",
    "\n",
    "      # Add loss and accuracy to their lists to calculate the average later.\n",
    "      lossHistory.append(test_loss.detach().item())\n",
    "      accuracyHistory.append(test_accuracy.detach())\n",
    "\n",
    "    # Calculate the average loss and accuracy for the test set.\n",
    "    avg_loss = sum(lossHistory) / len(lossHistory)\n",
    "    avg_accuracy = sum(accuracyHistory) / len(accuracyHistory)\n",
    "\n",
    "    # Print the test accuracy\n",
    "    print(f\"Test Accuracy: {int(torch.round(avg_accuracy * 100).item())}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Activation Function Experiments\n",
    "Experimenting with three different activation functions (ReLU, Sigmoid, Tanh) to assess:\n",
    "- Early learning speed: How quickly the network improves accuracy in the first few epochs.\n",
    "- Convergence behaviour: How smoothly training progresses over 15 epochs.\n",
    "- Final test performance: Accuracy on the unseen test set.\n",
    "This analysis highlights how activation choice affects both optimisation dynamics and generalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1716933290751,
     "user": {
      "displayName": "Cheryl C",
      "userId": "07083983319276283397"
     },
     "user_tz": -720
    },
    "id": "OwqedsLgCQbn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ReLU activation function:\n",
      "Model Training:\n",
      "Epoch 1: Loss = 2.288, Accuracy = 41%\n",
      "Epoch 2: Loss = 2.166, Accuracy = 59%\n",
      "Epoch 3: Loss = 1.888, Accuracy = 66%\n",
      "Epoch 4: Loss = 1.733, Accuracy = 80%\n",
      "Epoch 5: Loss = 1.664, Accuracy = 84%\n",
      "Epoch 6: Loss = 1.631, Accuracy = 86%\n",
      "Epoch 7: Loss = 1.580, Accuracy = 93%\n",
      "Epoch 8: Loss = 1.559, Accuracy = 94%\n",
      "Epoch 9: Loss = 1.544, Accuracy = 94%\n",
      "Epoch 10: Loss = 1.531, Accuracy = 95%\n",
      "Epoch 11: Loss = 1.522, Accuracy = 96%\n",
      "Epoch 12: Loss = 1.518, Accuracy = 96%\n",
      "Epoch 13: Loss = 1.510, Accuracy = 97%\n",
      "Epoch 14: Loss = 1.512, Accuracy = 96%\n",
      "Epoch 15: Loss = 1.502, Accuracy = 97%\n",
      "Test Accuracy: 95%\n",
      "\n",
      "Sigmoid activation function:\n",
      "Model Training:\n",
      "Epoch 1: Loss = 2.304, Accuracy = 9%\n",
      "Epoch 2: Loss = 2.300, Accuracy = 11%\n",
      "Epoch 3: Loss = 2.297, Accuracy = 11%\n",
      "Epoch 4: Loss = 2.290, Accuracy = 11%\n",
      "Epoch 5: Loss = 2.269, Accuracy = 15%\n",
      "Epoch 6: Loss = 2.224, Accuracy = 26%\n",
      "Epoch 7: Loss = 2.171, Accuracy = 35%\n",
      "Epoch 8: Loss = 2.106, Accuracy = 39%\n",
      "Epoch 9: Loss = 2.029, Accuracy = 57%\n",
      "Epoch 10: Loss = 1.955, Accuracy = 62%\n",
      "Epoch 11: Loss = 1.902, Accuracy = 64%\n",
      "Epoch 12: Loss = 1.869, Accuracy = 65%\n",
      "Epoch 13: Loss = 1.847, Accuracy = 65%\n",
      "Epoch 14: Loss = 1.835, Accuracy = 65%\n",
      "Epoch 15: Loss = 1.827, Accuracy = 66%\n",
      "Test Accuracy: 63%\n",
      "\n",
      "Tanh activation function:\n",
      "Model Training:\n",
      "Epoch 1: Loss = 2.266, Accuracy = 34%\n",
      "Epoch 2: Loss = 2.051, Accuracy = 68%\n",
      "Epoch 3: Loss = 1.782, Accuracy = 80%\n",
      "Epoch 4: Loss = 1.659, Accuracy = 89%\n",
      "Epoch 5: Loss = 1.592, Accuracy = 92%\n",
      "Epoch 6: Loss = 1.560, Accuracy = 94%\n",
      "Epoch 7: Loss = 1.539, Accuracy = 95%\n",
      "Epoch 8: Loss = 1.528, Accuracy = 95%\n",
      "Epoch 9: Loss = 1.520, Accuracy = 96%\n",
      "Epoch 10: Loss = 1.513, Accuracy = 97%\n",
      "Epoch 11: Loss = 1.511, Accuracy = 96%\n",
      "Epoch 12: Loss = 1.505, Accuracy = 97%\n",
      "Epoch 13: Loss = 1.500, Accuracy = 98%\n",
      "Epoch 14: Loss = 1.497, Accuracy = 97%\n",
      "Epoch 15: Loss = 1.494, Accuracy = 98%\n",
      "Test Accuracy: 96%\n"
     ]
    }
   ],
   "source": [
    "# Experimenting with Activation Functions: Comparing the effectiveness of ReLU, Sigmoid, and Tanh.\n",
    "\n",
    "\"\"\"\n",
    "Activation functions introduce non-linearity into the model.\n",
    "\n",
    "ReLU: (max(z, 0)), replaces any negative values with 0\n",
    "and preserves positive values; activates only the neurons\n",
    "with positive outputs to make the model sparse and efficient.\n",
    "\n",
    "Sigmoid: Maps any input z to a value between 0 and 1, which can be\n",
    "interpreted as probabilities.\n",
    "\n",
    "Tanh: Maps any input z to a value between -1 and 1, makes it zero-centered\n",
    "which can help center the data and make optimisation easier.\n",
    "\n",
    "ReLU and Tanh often used in hidden layers, Sigmoid in output layers for binary classfication.\n",
    "\"\"\"\n",
    "activation_functions = {\n",
    "    'ReLU': F.relu,\n",
    "    'Sigmoid': torch.sigmoid,\n",
    "    'Tanh': torch.tanh\n",
    "}\n",
    "\n",
    "# Train and test the network with each activation function\n",
    "for name, act_fnc in activation_functions.items():\n",
    "    print(f\"\\n{name} activation function:\")\n",
    "    # Instantiating the neural network using the previously defined NeuralNetwork class\n",
    "    model = NeuralNetwork(act_fnc)\n",
    "    # Defining the loss function\n",
    "    lossFunction = torch.nn.CrossEntropyLoss() # calculates how off predictions are from actual values\n",
    "    # Choosing an optimiser and an appropriate learning rate: chose Adam over SGD since Adam has adaptive learning rate for each parameter\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    # train_show will print all 15 epoch results\n",
    "    print(\"Model Training:\")\n",
    "    train_show(model, lossFunction, optimiser, 15)\n",
    "    # test_network will print average test accuracy\n",
    "    test_network(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Activation Function Comparison\n",
    "\n",
    "- *ReLU*: Fast early learning (41% -> 80% in the first 4 epochs) with steady convergence. Final training accuracy 97%, test accuracy 95%.\n",
    "- *Sigmoid*: Very slow early learning (<15% accuracy until epoch 5) due to vanishing gradients. Final training accuracy 66%, test accuracy 63%.\n",
    "- *Tanh*: Rapid early learning (34% -> 89% in the first 4 epochs) with smooth convergence. Final training accuracy 98%, test accuracy 96%, slightly outperforming ReLU.\n",
    "  \n",
    "**Conclusion**: For this MLP on the Digits dataset, **Tanh and ReLU are both effective**, with Tanh having a slight edge. Sigmoid underperforms due to slow convergence caused by vanishing gradients."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMmCVYFBBUtEU+P+95QVxsJ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
